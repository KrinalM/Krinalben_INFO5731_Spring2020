{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KrinalM/Krinalben_INFO5731_Spring2020/blob/main/Monpara_Krinalben_Exercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Research Question:\n",
        "\n",
        "What impact does teen use of social media have on mental health in urban settings?\n",
        "\n",
        "Data Collection:\n",
        "\n",
        "Define Variables: List important variables like the frequency, duration, and platforms used for social media use; mental health indicators like stress, depression, and anxiety; demographic data like age, gender, and socioeconomic status; and possible confounding variables like family dynamics and peer pressure.\n",
        "\n",
        "Survey Design: Create a thorough questionnaire including questions on social media usage and validated scales for assessing mental health indicators (such as the PHQ-9 for depression and the GAD-7 for anxiety). Make sure the questions are considerate to the intended audience, succinct, and clear.\n",
        "\n",
        "Sample Strategy: To guarantee representation across all age groups, genders, and socioeconomic backgrounds within metropolitan regions, employ a stratified random sample approach. To have enough statistical power, try to have a sample size of at least 500 people.\n",
        "\n",
        "Procedure for Gathering Data: Send out the survey electronically through community centers, social media sites, school networks, and other pertinent channels. Get participants' informed permission while maintaining their privacy and confidentiality.\n",
        "\n",
        "Data Management and Storage: Make use of safe, GDPR-compliant data storage solutions that adhere to data protection laws. To ensure security, encrypt critical data and provide each participant a special identification number. Make frequent data backups to guard against loss.\n",
        "\n",
        "Data Cleaning and Preprocessing: To find and fix any mistakes, discrepancies, or missing values in the dataset, do a comprehensive data cleaning. As needed for analysis, recode variables and standardize replies.\n",
        "\n",
        "Data analysis: While accounting for confounding variables, use statistical methods including regression analysis, correlation analysis, and structural equation modeling to investigate the associations between social media use and mental health outcomes.\n",
        "\n",
        "Reporting and Interpretation: Examine the results in the context of current research and theoretical frameworks. Talk about the implications for future research paths, intervention measures, and policy. Results should be presented at pertinent conferences or seminars and published in peer-reviewed publications."
      ],
      "metadata": {
        "id": "oK4W9qHv-hQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# Generate synthetic data for social media usage and mental health indicators\n",
        "data = []\n",
        "\n",
        "for _ in range(1000):\n",
        "    age = random.randint(13, 19)\n",
        "    gender = random.choice(['Male', 'Female'])\n",
        "    socio_economic_status = random.choice(['Low', 'Middle', 'High'])\n",
        "    social_media_usage = random.randint(0, 10)  # Assuming a scale of 0 to 10 for frequency/duration\n",
        "    stress_level = random.randint(1, 10)  # Assuming a scale of 1 to 10 for stress level\n",
        "    depression_score = random.randint(0, 27)  # PHQ-9 scale ranges from 0 to 27\n",
        "    anxiety_score = random.randint(0, 21)  # GAD-7 scale ranges from 0 to 21\n",
        "\n",
        "    data.append([age, gender, socio_economic_status, social_media_usage, stress_level, depression_score, anxiety_score])\n",
        "\n",
        "# Create a DataFrame\n",
        "columns = ['Age', 'Gender', 'Socioeconomic Status', 'Social Media Usage', 'Stress Level', 'Depression Score', 'Anxiety Score']\n",
        "df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "# Save DataFrame to CSV file\n",
        "df.to_csv('social_media_mental_health_dataset.csv', index=False)\n",
        "\n",
        "print(\"Dataset saved successfully.\")"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e2e9632-5b7b-4346-ec7d-69e29f42dfa1"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e0d18a3-d6e7-44e1-ca23-9528dbf11b33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Articles saved successfully.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def scrape_google_scholar(keyword, start_year, end_year, num_articles):\n",
        "    base_url = \"https://scholar.google.com/scholar\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
        "    params = {\n",
        "        \"q\": keyword,\n",
        "        \"as_ylo\": start_year,\n",
        "        \"as_yhi\": end_year,\n",
        "        \"hl\": \"en\",\n",
        "        \"as_sdt\": \"0,5\",\n",
        "        \"num\": 10  # Scraping 10 articles per page\n",
        "    }\n",
        "\n",
        "    articles = []\n",
        "\n",
        "    for page in range(num_articles // 10):\n",
        "        params[\"start\"] = page * 10\n",
        "        response = requests.get(base_url, params=params, headers=headers)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        for result in soup.find_all('div', class_='gs_r'):\n",
        "            title_tag = result.find('h3', class_='gs_rt')\n",
        "            title = title_tag.text.strip() if title_tag else \"N/A\"\n",
        "\n",
        "            venue_tag = result.find('div', class_='gs_a')\n",
        "            venue_year = venue_tag.text.split('-') if venue_tag else [\"N/A\", \"N/A\"]\n",
        "            venue = venue_year[0].strip()\n",
        "            year = venue_year[-1].strip()\n",
        "\n",
        "            authors_tag = result.find('div', class_='gs_a')\n",
        "            authors = authors_tag.text.split('-')[1].strip() if authors_tag else \"N/A\"\n",
        "\n",
        "            abstract_tag = result.find('div', class_='gs_rs')\n",
        "            abstract = abstract_tag.text.strip() if abstract_tag else \"N/A\"\n",
        "\n",
        "            articles.append([title, venue, year, authors, abstract])\n",
        "\n",
        "        time.sleep(1)  # Adding a smaller delay between requests\n",
        "\n",
        "    return articles\n",
        "\n",
        "# Parameters\n",
        "keyword = \"XYZ\"\n",
        "start_year = 2014\n",
        "end_year = 2024\n",
        "num_articles = 1000\n",
        "\n",
        "# Scraping Google Scholar\n",
        "articles = scrape_google_scholar(keyword, start_year, end_year, num_articles)\n",
        "\n",
        "# Creating DataFrame\n",
        "df = pd.DataFrame(articles, columns=['Title', 'Venue/Journal/Conference', 'Year', 'Authors', 'Abstract'])\n",
        "\n",
        "# Saving DataFrame to CSV\n",
        "df.to_csv('google_scholar_articles.csv', index=False)\n",
        "\n",
        "print(\"Articles saved successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "I57NXsauCec2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "8d3bc324-46e5-4e8b-d603-e59910029e6a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nI used online tool ParseHub fro data extraxtion.\\n\\nSELECTED TOOL:\\nParseHub is a powerful and user-friendly web scraping tool that allows you to extract data from websites easily.\\n\\nSTEPS FOR WEB SCRAPPING:\\nDownload the ParseHub app.\\nOpened ParseHub and clicked on \"New Project\". Entered the URL of the website (https://www.imdb.com/chart/top/) for scrapping.\\nSet up Selectors\\nRefined Selections as needed\\nClicked on the \"Get Data\" button to start the scraping process. ParseHub loaded the page and extracted the data.\\nOnce the scraping is completed, reviewed the extracted data in the ParseHub interface. And then exported it in formats like CSV.\\n\\nBelow is the link to the CSV with data collected using ParseHub:\\n\\nLink: https://myunt-my.sharepoint.com/:x:/g/personal/krinalbenmonpara_my_unt_edu/EcABEDyXVqRMsCWkBUiY3LUBl5e-mNm9qGZfOGvpDSHGNg?e=SuWa6L\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "'''\n",
        "I used online tool ParseHub fro data extraxtion.\n",
        "\n",
        "SELECTED TOOL:\n",
        "ParseHub is a powerful and user-friendly web scraping tool that allows you to extract data from websites easily.\n",
        "\n",
        "STEPS FOR WEB SCRAPPING:\n",
        "Download the ParseHub app.\n",
        "Opened ParseHub and clicked on \"New Project\". Entered the URL of the website (https://www.imdb.com/chart/top/) for scrapping.\n",
        "Set up Selectors\n",
        "Refined Selections as needed\n",
        "Clicked on the \"Get Data\" button to start the scraping process. ParseHub loaded the page and extracted the data.\n",
        "Once the scraping is completed, reviewed the extracted data in the ParseHub interface. And then exported it in formats like CSV.\n",
        "\n",
        "Below is the link to the CSV with data collected using ParseHub:\n",
        "\n",
        "Link: https://myunt-my.sharepoint.com/:x:/g/personal/krinalbenmonpara_my_unt_edu/EcABEDyXVqRMsCWkBUiY3LUBl5e-mNm9qGZfOGvpDSHGNg?e=SuWa6L\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "To be honest,\n",
        "The learning experience in web scraping involves a combination of technical skills, domain knowledge, and ethical considerations.\n",
        "Practicing on various online sources and real-world projects helps in honing these skills and becoming proficient in extracting data from the web.\n",
        "Question 4 was very challenging for me. Used ParseHub for data extraction.\n",
        "the ability to gather and analyze data from online sources enhances research,\n",
        "decision-making, and insights across diverse fields, making web scraping a valuable tool for professionals and academics alike.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "4435b7c6-24e4-49f8-da9e-f400eb11a268"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTo be honest, \\nThe learning experience in web scraping involves a combination of technical skills, domain knowledge, and ethical considerations. \\nPracticing on various online sources and real-world projects helps in honing these skills and becoming proficient in extracting data from the web.\\nQuestion 4 was very challenging for me. Used ParseHub for data extraction.\\nthe ability to gather and analyze data from online sources enhances research, \\ndecision-making, and insights across diverse fields, making web scraping a valuable tool for professionals and academics alike.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}