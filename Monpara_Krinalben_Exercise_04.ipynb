{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KrinalM/Krinalben_INFO5731_Spring2020/blob/main/Monpara_Krinalben_Exercise_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 4**\n",
        "\n",
        "**This exercise will provide a valuable learning experience in working with text data and extracting features using various topic modeling algorithms. Key concepts such as Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA), lda2vec, and BERTopic.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ],
      "metadata": {
        "id": "TU-pLW33lpcS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "\n",
        "**Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "VAZj4PHB70nf"
      },
      "outputs": [],
      "source": [
        "# Required Libraries\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import CoherenceModel\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Text Corpus\n",
        "documents = [\n",
        "    \"The product is really good. I love it!\",\n",
        "    \"This service is terrible. I'm never using it again.\",\n",
        "    \"The quality of the item is poor. I'm disappointed.\",\n",
        "    \"The customer support was excellent. They were very helpful.\",\n",
        "    \"Not recommended. Waste of money.\",\n",
        "]"
      ],
      "metadata": {
        "id": "pNHM_yBt10wf"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Preprocess the text data\n",
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "jjzAzF1S1-MB"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(texts):\n",
        "    return [[token.lemma_ for token in nlp(text) if not token.is_stop and not token.is_punct] for text in texts]\n",
        "\n",
        "processed_data = preprocess_text(documents)"
      ],
      "metadata": {
        "id": "0w8Jd35c2GwF"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Create a dictionary and document-term matrix\n",
        "id2word = corpora.Dictionary(processed_data)\n",
        "corpus = [id2word.doc2bow(text) for text in processed_data]"
      ],
      "metadata": {
        "id": "XOU_G5nK2az_"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Determine the optimal number of topics (K) using coherence scores\n",
        "coherence_scores = [(k, CoherenceModel(model=gensim.models.LdaModel(corpus=corpus,\n",
        "                                                                   id2word=id2word,\n",
        "                                                                    num_topics=k,\n",
        "                                                                    random_state=100,\n",
        "                                                                    update_every=1,\n",
        "                                                                    chunksize=100,\n",
        "                                                                    passes=10,\n",
        "                                                                    alpha='auto',\n",
        "                                                                    per_word_topics=True), texts=processed_data, dictionary=id2word, coherence='c_v').get_coherence()) for k in range(2, 11)]\n"
      ],
      "metadata": {
        "id": "nwEE8Cfs2k-2"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the K with the highest coherence score\n",
        "best_k = None\n",
        "best_coherence = float(\"-inf\")\n",
        "\n",
        "for k, coherence in coherence_scores:\n",
        "    if coherence > best_coherence:\n",
        "        best_k = k\n",
        "        best_coherence = coherence\n",
        "\n",
        "print(f\"Optimal number of topics (K): {best_k}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmpW6Ex13U80",
        "outputId": "2123d5fb-1766-401b-c25f-f4fc476e51b4"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal number of topics (K): 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Train the LDA model with the selected K\n",
        "lda_model = gensim.models.LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=id2word,\n",
        "    num_topics=best_k,\n",
        "    random_state=100,\n",
        "    update_every=1,\n",
        "    chunksize=100,\n",
        "    passes=10,\n",
        "    alpha='auto',\n",
        "    per_word_topics=True\n",
        ")"
      ],
      "metadata": {
        "id": "LJobki5G3e2A"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Summarize the topics\n",
        "topics = lda_model.print_topics(num_topics=best_k, num_words=5)\n",
        "for topic in topics:\n",
        "    print(f\"Topic {topic[0] + 1}: {topic[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo0DM1P93nFf",
        "outputId": "ef8f4670-ba35-4f78-c3a5-d64dca68f11d"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1: 0.125*\"money\" + 0.125*\"waste\" + 0.125*\"love\" + 0.125*\"recommend\" + 0.125*\"product\"\n",
            "Topic 2: 0.208*\"terrible\" + 0.208*\"service\" + 0.042*\"good\" + 0.042*\"love\" + 0.042*\"recommend\"\n",
            "Topic 3: 0.156*\"quality\" + 0.156*\"disappointed\" + 0.156*\"poor\" + 0.156*\"item\" + 0.031*\"good\"\n",
            "Topic 4: 0.156*\"excellent\" + 0.156*\"support\" + 0.156*\"customer\" + 0.156*\"helpful\" + 0.031*\"good\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "\n",
        "**Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "EoQX5s4O70nf"
      },
      "outputs": [],
      "source": [
        "# Required Libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Preprocess the text data and create a TF-IDF matrix\n",
        "def preprocess_text_data(data):\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(data)\n",
        "    return tfidf_matrix\n",
        "\n",
        "# Sample Text Corpus\n",
        "documents = [\n",
        "    \"The product is really good. I love it!\",\n",
        "    \"This service is terrible. I'm never using it again.\",\n",
        "    \"The quality of the item is poor. I'm disappointed.\",\n",
        "    \"The customer support was excellent. They were very helpful.\",\n",
        "    \"Not recommended. Waste of money.\",\n",
        "]\n",
        "\n",
        "tfidf_matrix = preprocess_text_data(documents)"
      ],
      "metadata": {
        "id": "Uhg29jAy8amL"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Determine the optimal number of topics (K) using LSA (you specify K)\n",
        "def determine_optimal_topics(tfidf_matrix, K):\n",
        "    lsa = TruncatedSVD(n_components=K)\n",
        "    lsa_topic_matrix = lsa.fit_transform(tfidf_matrix)\n",
        "    return lsa_topic_matrix\n",
        "\n",
        "# Choose the number of topics\n",
        "num_topics = 5\n",
        "lsa_topic_matrix = determine_optimal_topics(tfidf_matrix, num_topics)"
      ],
      "metadata": {
        "id": "C5ssgo8Y8s_L"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Summarize the topics\n",
        "terms = tfidf_vectorizer.get_feature_names_out()\n",
        "topic_keywords = [[terms[idx] for idx in topic.argsort()[-5:][::-1]] for topic in lsa.components_]\n",
        "for i, top_terms in enumerate(topic_keywords):\n",
        "    print(f\"Topic {i + 1}: {', '.join(top_terms)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCkE6fvu86r2",
        "outputId": "69da428f-da1f-4841-c528-e7ed0312e511"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1: the, is, it, really, good\n",
            "Topic 2: of, money, not, waste, recommended\n",
            "Topic 3: customer, excellent, helpful, were, was\n",
            "Topic 4: again, service, using, this, never\n",
            "Topic 5: good, really, love, product, it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "**Generate K topics by using lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "2CRuXfV570ng"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "**Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "b4HoWK-i70ng"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 3 (Alternative) - (10 points)**\n",
        "\n",
        "If you are unable to do the topic modeling using lda2vec, do the alternate question.\n",
        "\n",
        "Provide atleast 3 visualization for the topics generated by the BERTopic or LDA model. Explain each of the visualization in detail."
      ],
      "metadata": {
        "id": "Wslk2SYHML8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "# Then Explain the visualization\n",
        "\n",
        "# Repeat for the other 2 visualizations as well."
      ],
      "metadata": {
        "id": "eKZHcPjpNEDx"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Question (5 Points)\n",
        "\n",
        "**Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details.**\n",
        "\n",
        "**This question will compensate for any points deducted in this exercise. Maximum marks for the exercise is 40 points.**"
      ],
      "metadata": {
        "id": "d89ODUx3jjJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Examining the outcomes produced by two distinct topic modeling algorithms, namely Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA),\n",
        "entails taking into account a number of variables, including the coherence score, the interpretability of the topics,\n",
        "computational efficiency, and domain-specific requirements.\n",
        "\n",
        "The \"better\" method is determined by the particular needs of the job based on the evaluation.\n",
        "Because of its interpretability, coherence, and processing economy, LDA is a good option.\n",
        "\n",
        "\n",
        "LDA - The coherence score aids in figuring out the ideal quantity of subjects.\n",
        "subjects with relatively high coherence scores, which indicate cohesive and separate subjects, are frequently produced via LDA.\n",
        "Since each subject is represented by a distribution of words, LDA usually produces highly interpretable topics.\n",
        "These subjects are frequently simple to comprehend and evaluate.Large corpora can be easily scaled using LDA's computational efficiency.\n",
        "For subject modeling jobs, it is commonly utilized. It is a flexible technique that works well for a wide range of text mining applications\n",
        "and is extensively used in both academia and business.\n",
        "\n",
        "\n",
        "LSA - LSA uses coherence scores to calculate the ideal number of topics, much like LDA does.\n",
        "However, because LSA depends on singular value decomposition (SVD), it could not yield coherence scores as well as LDA.\n",
        "Considering that LSA depends on linear algebra transformations that could not adequately maintain semantic links,\n",
        "the topics it generates might be less interpretable than those produced by LDA.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "OK34nZtojhmm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "68c493ce-638b-4e12-8e33-fff26869ed22"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nExamining the outcomes produced by two distinct topic modeling algorithms, namely Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA), \\nentails taking into account a number of variables, including the coherence score, the interpretability of the topics, \\ncomputational efficiency, and domain-specific requirements.\\n\\nThe \"better\" method is determined by the particular needs of the job based on the evaluation. \\nBecause of its interpretability, coherence, and processing economy, LDA is a good option.\\n\\n\\nLDA - The coherence score aids in figuring out the ideal quantity of subjects. \\nsubjects with relatively high coherence scores, which indicate cohesive and separate subjects, are frequently produced via LDA.\\nSince each subject is represented by a distribution of words, LDA usually produces highly interpretable topics. \\nThese subjects are frequently simple to comprehend and evaluate.Large corpora can be easily scaled using LDA\\'s computational efficiency. \\nFor subject modeling jobs, it is commonly utilized. It is a flexible technique that works well for a wide range of text mining applications \\nand is extensively used in both academia and business.\\n\\n\\nLSA - LSA uses coherence scores to calculate the ideal number of topics, much like LDA does. \\nHowever, because LSA depends on singular value decomposition (SVD), it could not yield coherence scores as well as LDA.\\nConsidering that LSA depends on linear algebra transformations that could not adequately maintain semantic links, \\nthe topics it generates might be less interpretable than those produced by LDA.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "Consider the following points in your response:\n",
        "\n",
        "**Learning Experience:** Describe your overall learning experience in working with text data and extracting features using various topic modeling algorithms. Did you understand these algorithms and did the implementations helped in grasping the nuances of feature extraction from text data.\n",
        "\n",
        "**Challenges Encountered:** Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "Overall, exercise was very difficult. I have done Question 1 and 2. Those are little bit easy to code.\n",
        "But question 3 and 4 is very tough and I also encountered challenges while working.\n",
        "So, Unfortunately, I couldn't completed Question 3 and 4. But, still I'm working on it and try to solve soon.\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7368587b-55ab-4a5f-b476-21d34cfa4cbf"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nPlease write you answer here:\\n\\nOverall, exercise was very difficult. I have done Question 1 and 2. Those are little bit easy to code.\\nBut question 3 and 4 is very tough and I also encountered challenges while working.\\nSo, Unfortunately, I couldn't completed Question 3 and 4. But, still I'm working on it and try to solve soon.\\n\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ARqm7u6B70ne",
        "dEUjBE6C70nf",
        "7oSK4soH70nf"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}