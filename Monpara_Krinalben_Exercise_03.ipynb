{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KrinalM/Krinalben_INFO5731_Spring2020/blob/main/Monpara_Krinalben_Exercise_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf1b7600-0c26-4417-84d0-c7304f8fd2d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nSentiment analysis of customer evaluations for a product or service might be one intriguing text classification assignment. \\nSentiment analysis is the process of identifying the sentiment—which can be neutral, positive, or negative—expressed in a text. \\nBusinesses may learn about client satisfaction levels and pinpoint areas for development by completing this activity.\\n\\nMany kinds of characteristics may be used to create a machine learning model for sentiment analysis:\\n\\nBag-of-Words (BoW) Features: Text is converted into a vector of word frequencies in order to depict BoW. \\n                             Every word in the lexicon becomes a feature, and the value of a feature is determined by how frequently it appears in the document.\\n                             BoW records the words that appear in the text and can offer important insights into emotion. \\n                             Positive terms like \"good,\" \"excellent,\" or negative terms like \"bad,\" \"poor,\" for instance, may be very telling of attitude.\\n                \\nPart-of-Speech (POS) Features: Labeling each word in a text with the appropriate part of speech—a noun, verb, adjective, etc.—is known as POS tagging. \\n                               Sentiment-influencing grammatical structures and syntactic patterns can be captured via POS characteristics. \\n                               For example, adverbs and adjectives are frequently effective markers of sentiment in a statement. \\n                               Understanding the sentiment conveyed in the text may be gained by examining the distribution of POS tags.\\n\\nTF-IDF Features: A statistical metric called Term Frequency-Inverse Document Frequency (TF-IDF) assesses a word\\'s significance in a document in relation to a group of documents.\\n                 TF-IDF takes into account a term\\'s rarity throughout texts (IDF) as well as its frequency in a document (TF). \\n                 This feature representation may assist capture sentiment-bearing phrases more successfully by assigning greater weight to terms that occur frequently in a text but infrequently over the whole corpus.\\n\\nN-grams Features: Contiguous groups of n words from a given text are called N-grams.     \\n                  N-grams are able to record more contextual information since they take word sequences into account rather than simply individual words. \\n                  N-gram-derived features are capable of capturing the subtleties of sentiment conveyed by word choices or phrases. \\n                  Bigrams that express feeling beyond single words, such \"not good\" or \"very helpful,\" are one example.\\n\\nSentiment Lexicon Features: Sentiment lexicons are dictionaries that have words marked with the positive, negative, or neutral sentiment polarity. \\n                            Matching words in the text with lexicon entries and utilizing their sentiment labels as features is known as \"leveraging sentiment lexicons as features.\"\\n                            Through the explicit use of domain-specific knowledge about words that carry sentiment, this strategy improves the model\\'s performance in sentiment classification tasks.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 140
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Sentiment analysis of customer evaluations for a product or service might be one intriguing text classification assignment.\n",
        "Sentiment analysis is the process of identifying the sentiment—which can be neutral, positive, or negative—expressed in a text.\n",
        "Businesses may learn about client satisfaction levels and pinpoint areas for development by completing this activity.\n",
        "\n",
        "Many kinds of characteristics may be used to create a machine learning model for sentiment analysis:\n",
        "\n",
        "Bag-of-Words (BoW) Features: Text is converted into a vector of word frequencies in order to depict BoW.\n",
        "                             Every word in the lexicon becomes a feature, and the value of a feature is determined by how frequently it appears in the document.\n",
        "                             BoW records the words that appear in the text and can offer important insights into emotion.\n",
        "                             Positive terms like \"good,\" \"excellent,\" or negative terms like \"bad,\" \"poor,\" for instance, may be very telling of attitude.\n",
        "\n",
        "Part-of-Speech (POS) Features: Labeling each word in a text with the appropriate part of speech—a noun, verb, adjective, etc.—is known as POS tagging.\n",
        "                               Sentiment-influencing grammatical structures and syntactic patterns can be captured via POS characteristics.\n",
        "                               For example, adverbs and adjectives are frequently effective markers of sentiment in a statement.\n",
        "                               Understanding the sentiment conveyed in the text may be gained by examining the distribution of POS tags.\n",
        "\n",
        "TF-IDF Features: A statistical metric called Term Frequency-Inverse Document Frequency (TF-IDF) assesses a word's significance in a document in relation to a group of documents.\n",
        "                 TF-IDF takes into account a term's rarity throughout texts (IDF) as well as its frequency in a document (TF).\n",
        "                 This feature representation may assist capture sentiment-bearing phrases more successfully by assigning greater weight to terms that occur frequently in a text but infrequently over the whole corpus.\n",
        "\n",
        "N-grams Features: Contiguous groups of n words from a given text are called N-grams.\n",
        "                  N-grams are able to record more contextual information since they take word sequences into account rather than simply individual words.\n",
        "                  N-gram-derived features are capable of capturing the subtleties of sentiment conveyed by word choices or phrases.\n",
        "                  Bigrams that express feeling beyond single words, such \"not good\" or \"very helpful,\" are one example.\n",
        "\n",
        "Sentiment Lexicon Features: Sentiment lexicons are dictionaries that have words marked with the positive, negative, or neutral sentiment polarity.\n",
        "                            Matching words in the text with lexicon entries and utilizing their sentiment labels as features is known as \"leveraging sentiment lexicons as features.\"\n",
        "                            Through the explicit use of domain-specific knowledge about words that carry sentiment, this strategy improves the model's performance in sentiment classification tasks.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow import keras\n",
        "from typing import List\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "GO1CUSYVEzvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoQX5s4O70nf"
      },
      "outputs": [],
      "source": [
        "# Sample text data\n",
        "texts = [\n",
        "    \"The product is really good. I love it!\",\n",
        "    \"This service is terrible. I'm never using it again.\",\n",
        "    \"The quality of the item is poor. I'm disappointed.\",\n",
        "    \"The customer support was excellent. They were very helpful.\",\n",
        "    \"Not recommended. Waste of money.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag-of-Words (BoW) Features\n",
        "def print_bow(texts: List[str]) -> None:\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    word_index = tokenizer.word_index\n",
        "    bow = {}\n",
        "    for key in word_index:\n",
        "        bow[key] = sequences[0].count(word_index[key])\n",
        "\n",
        "    print(f\"Bag of word sentence 1:\\n{bow}\")\n",
        "    print(f\"We found {len(word_index)} unique tokens.\")\n",
        "\n",
        "print_bow(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW04I5cdKDv5",
        "outputId": "4162ce4b-2b36-4be7-cc0e-56d95da31f4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of word sentence 1:\n",
            "{'the': 1, 'is': 1, 'it': 1, \"i'm\": 0, 'of': 0, 'product': 1, 'really': 1, 'good': 1, 'i': 1, 'love': 1, 'this': 0, 'service': 0, 'terrible': 0, 'never': 0, 'using': 0, 'again': 0, 'quality': 0, 'item': 0, 'poor': 0, 'disappointed': 0, 'customer': 0, 'support': 0, 'was': 0, 'excellent': 0, 'they': 0, 'were': 0, 'very': 0, 'helpful': 0, 'not': 0, 'recommended': 0, 'waste': 0, 'money': 0}\n",
            "We found 32 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "tokenized_texts = [word_tokenize(text.lower()) for text in texts]\n",
        "# Part-of-Speech (POS) Features\n",
        "pos_features = []\n",
        "for text in tokenized_texts:\n",
        "    pos_tags = [tag for word, tag in pos_tag(text)]\n",
        "    pos_features.append(pos_tags)\n",
        "print(\"POS Features:\")\n",
        "print(pos_features)  # Print the POS tag sequences for each text\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgVv2mJCEVXX",
        "outputId": "221e6b73-08a3-4b6b-f0bb-8f28fadc8d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Features:\n",
            "[['DT', 'NN', 'VBZ', 'RB', 'JJ', '.', 'VB', 'VBP', 'PRP', '.'], ['DT', 'NN', 'VBZ', 'JJ', '.', 'NN', 'VBP', 'RB', 'VBG', 'PRP', 'RB', '.'], ['DT', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'JJ', '.', 'JJ', 'VBP', 'JJ', '.'], ['DT', 'NN', 'NN', 'VBD', 'JJ', '.', 'PRP', 'VBD', 'RB', 'JJ', '.'], ['RB', 'VBN', '.', 'NN', 'IN', 'NN', '.']]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Features\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(texts)\n",
        "print(\"TF-IDF Features:\")\n",
        "print(tfidf_vectorizer.get_feature_names_out())  # Print the feature names\n",
        "print(tfidf_features.toarray())  # Print the TF-IDF feature vectors\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6JsMFQUEgNX",
        "outputId": "9b0d535c-052a-43d8-c113-0eb7faec4df7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Features:\n",
            "['again' 'customer' 'disappointed' 'excellent' 'good' 'helpful' 'is' 'it'\n",
            " 'item' 'love' 'money' 'never' 'not' 'of' 'poor' 'product' 'quality'\n",
            " 'really' 'recommended' 'service' 'support' 'terrible' 'the' 'they' 'this'\n",
            " 'using' 'very' 'was' 'waste' 'were']\n",
            "[[0.         0.         0.         0.         0.42455503 0.\n",
            "  0.28432945 0.34252832 0.         0.42455503 0.         0.\n",
            "  0.         0.         0.         0.42455503 0.         0.42455503\n",
            "  0.         0.         0.         0.         0.28432945 0.\n",
            "  0.         0.         0.         0.         0.         0.        ]\n",
            " [0.37530838 0.         0.         0.         0.         0.\n",
            "  0.2513484  0.30279644 0.         0.         0.         0.37530838\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.37530838 0.         0.37530838 0.         0.\n",
            "  0.37530838 0.37530838 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.38087336 0.         0.         0.\n",
            "  0.25507533 0.         0.38087336 0.         0.         0.\n",
            "  0.         0.30728623 0.38087336 0.         0.38087336 0.\n",
            "  0.         0.         0.         0.         0.51015065 0.\n",
            "  0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.34404072 0.         0.34404072 0.         0.34404072\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.34404072 0.         0.23040808 0.34404072\n",
            "  0.         0.         0.34404072 0.34404072 0.         0.34404072]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.46369322 0.\n",
            "  0.46369322 0.37410477 0.         0.         0.         0.\n",
            "  0.46369322 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.46369322 0.        ]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# N-grams Features\n",
        "n = 2  # for bigrams\n",
        "ngram_features = []\n",
        "for text in tokenized_texts:\n",
        "    bigrams = list(ngrams(text, n))\n",
        "    ngram_features.append([\" \".join(bigram) for bigram in bigrams])\n",
        "print(\"N-grams Features:\")\n",
        "print(ngram_features)  # Print the gene rated n-grams for each text\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNEpxQmWEkOM",
        "outputId": "e5633c79-8891-4768-db62-ee1a7f646b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N-grams Features:\n",
            "[['the product', 'product is', 'is really', 'really good', 'good .', '. i', 'i love', 'love it', 'it !'], ['this service', 'service is', 'is terrible', 'terrible .', '. i', \"i 'm\", \"'m never\", 'never using', 'using it', 'it again', 'again .'], ['the quality', 'quality of', 'of the', 'the item', 'item is', 'is poor', 'poor .', '. i', \"i 'm\", \"'m disappointed\", 'disappointed .'], ['the customer', 'customer support', 'support was', 'was excellent', 'excellent .', '. they', 'they were', 'were very', 'very helpful', 'helpful .'], ['not recommended', 'recommended .', '. waste', 'waste of', 'of money', 'money .']]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment Lexicon Features\n",
        "sentiment_lexicon = {\n",
        "    \"good\": \"positive\",\n",
        "    \"love\": \"positive\",\n",
        "    \"terrible\": \"negative\",\n",
        "    \"poor\": \"negative\",\n",
        "    \"excellent\": \"positive\",\n",
        "    \"helpful\": \"positive\",\n",
        "    \"recommended\": \"positive\",\n",
        "    \"waste\": \"negative\"\n",
        "}\n",
        "\n",
        "sentiment_lexicon_features = []\n",
        "for text in tokenized_texts:\n",
        "    sentiment_words = [word for word in text if word in sentiment_lexicon]\n",
        "    sentiment_labels = [sentiment_lexicon[word] for word in sentiment_words]\n",
        "    sentiment_lexicon_features.append(sentiment_labels)\n",
        "print(\"Sentiment Lexicon Features:\")\n",
        "print(sentiment_lexicon_features)  # Print the sentiment labels for sentiment words in each text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkpNjHuIEnNP",
        "outputId": "ec9a5800-7164-4663-aff1-33601ef2f360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Lexicon Features:\n",
            "[['positive', 'positive'], ['negative'], ['negative'], ['positive', 'positive'], ['positive', 'negative']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0db920d-e993-431f-da8b-5cb5c15842a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features ranked by TF-IDF scores:\n",
            "the: 1.0248881805932226\n",
            "is: 0.7907531744687686\n",
            "of: 0.681391000080969\n",
            "it: 0.6453247622448499\n",
            "money: 0.4636932227319092\n",
            "not: 0.4636932227319092\n",
            "recommended: 0.4636932227319092\n",
            "waste: 0.4636932227319092\n",
            "good: 0.4245550254370497\n",
            "love: 0.4245550254370497\n",
            "product: 0.4245550254370497\n",
            "really: 0.4245550254370497\n",
            "disappointed: 0.38087335870660655\n",
            "item: 0.38087335870660655\n",
            "poor: 0.38087335870660655\n",
            "quality: 0.38087335870660655\n",
            "again: 0.3753083838272226\n",
            "never: 0.3753083838272226\n",
            "service: 0.3753083838272226\n",
            "terrible: 0.3753083838272226\n",
            "this: 0.3753083838272226\n",
            "using: 0.3753083838272226\n",
            "customer: 0.34404071666393615\n",
            "excellent: 0.34404071666393615\n",
            "helpful: 0.34404071666393615\n",
            "support: 0.34404071666393615\n",
            "they: 0.34404071666393615\n",
            "very: 0.34404071666393615\n",
            "was: 0.34404071666393615\n",
            "were: 0.34404071666393615\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Compute the TF-IDF scores for each feature\n",
        "tfidf_scores = np.sum(tfidf_features, axis=0).tolist()[0]\n",
        "\n",
        "# Create a dictionary mapping feature names to their TF-IDF scores\n",
        "feature_tfidf_scores = dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_scores))\n",
        "\n",
        "# Sort the features based on their TF-IDF scores in descending order\n",
        "sorted_features = sorted(feature_tfidf_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the sorted features\n",
        "print(\"Features ranked by TF-IDF scores:\")\n",
        "for feature, score in sorted_features:\n",
        "    print(f\"{feature}: {score}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwD7bhFMZ26R",
        "outputId": "b4801f53-c818-4e79-b717-9c348ec6ac86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.38.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "485cb366-abbf-4b7d-c5d0-94a1e8dc861f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Texts Based on Similarity:\n",
            "Similarity: 0.7898 - Text: The customer support was excellent. They were very helpful.\n",
            "Similarity: 0.7636 - Text: The product is really good. I love it!\n",
            "Similarity: 0.3404 - Text: The quality of the item is poor. I'm disappointed.\n",
            "Similarity: 0.1495 - Text: Not recommended. Waste of money.\n",
            "Similarity: 0.1000 - Text: This service is terrible. I'm never using it again.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Sample text data\n",
        "texts = [\n",
        "    \"The product is really good. I love it!\",\n",
        "    \"This service is terrible. I'm never using it again.\",\n",
        "    \"The quality of the item is poor. I'm disappointed.\",\n",
        "    \"The customer support was excellent. They were very helpful.\",\n",
        "    \"Not recommended. Waste of money.\",\n",
        "]\n",
        "\n",
        "# Query\n",
        "query = \"I'm looking for a product with excellent customer support.\"\n",
        "\n",
        "# Load pre-trained BERT model\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# Generate embeddings for text data and query\n",
        "text_embeddings = model.encode(texts)\n",
        "query_embedding = model.encode([query])[0]\n",
        "\n",
        "# Calculate cosine similarity between query and text data\n",
        "similarities = cosine_similarity([query_embedding], text_embeddings)[0]\n",
        "\n",
        "# Rank texts based on similarity scores\n",
        "ranked_texts = sorted(\n",
        "    list(zip(texts, similarities)),\n",
        "    key=lambda x: x[1],\n",
        "    reverse=True\n",
        ")\n",
        "\n",
        "# Print ranked texts\n",
        "print(\"Ranked Texts Based on Similarity:\")\n",
        "for text, similarity in ranked_texts:\n",
        "    print(f\"Similarity: {similarity:.4f} - Text: {text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        " Working on extracting features from text data was a valuable learning experience.\n",
        " It reinforced the importance of feature engineering in natural language processing (NLP) tasks and\n",
        " highlighted various techniques for representing text data in a machine-readable format.\n",
        "\n",
        "The key concepts that I found most beneficial in understanding the feature extraction process:\n",
        "Bag-of-Words (BoW) representation\n",
        "TF-IDF\n",
        "Part-of-Speech (POS) tagging\n",
        "Sentiment Lexicon features\n",
        "\n",
        "One challenge I encountered was understanding and implementing feature selection techniques mentioned in the paper by Deng et al. (2019).\n",
        "While the exercise focused on using TF-IDF scores for feature ranking, exploring other statistical measures like\n",
        "Information Gain or Chi-square could provide a more comprehensive understanding of feature selection in text classification tasks.\n",
        "\n",
        "This exercise is highly relevant to the field of Natural Language Processing (NLP).\n",
        "Feature extraction plays a crucial role in NLP tasks such as sentiment analysis, text classification, and information retrieval.\n",
        "By learning how to extract and select relevant features from text data,\n",
        "NLP practitioners can improve the performance and interpretability of their machine learning models.\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "2b364d75-e7af-40a7-a2bc-d9f5fde32ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n Working on extracting features from text data was a valuable learning experience. \\n It reinforced the importance of feature engineering in natural language processing (NLP) tasks and \\n highlighted various techniques for representing text data in a machine-readable format. \\n\\nThe key concepts that I found most beneficial in understanding the feature extraction process:\\nBag-of-Words (BoW) representation\\nTF-IDF\\nPart-of-Speech (POS) tagging\\nSentiment Lexicon features\\n\\nOne challenge I encountered was understanding and implementing feature selection techniques mentioned in the paper by Deng et al. (2019).\\nWhile the exercise focused on using TF-IDF scores for feature ranking, exploring other statistical measures like \\nInformation Gain or Chi-square could provide a more comprehensive understanding of feature selection in text classification tasks.\\n\\nThis exercise is highly relevant to the field of Natural Language Processing (NLP). \\nFeature extraction plays a crucial role in NLP tasks such as sentiment analysis, text classification, and information retrieval. \\nBy learning how to extract and select relevant features from text data, \\nNLP practitioners can improve the performance and interpretability of their machine learning models.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ARqm7u6B70ne",
        "dEUjBE6C70nf",
        "7oSK4soH70nf",
        "7nZGAOwl70ng",
        "VEs-OoDEhTW4"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}