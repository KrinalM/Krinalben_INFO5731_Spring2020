{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KrinalM/Krinalben_INFO5731_Spring2020/blob/main/Monpara_Krinalben_Assignment_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Wednesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "!pip install semanticscholar\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from semanticscholar import SemanticScholar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZ9fAomjiZ8m",
        "outputId": "5c1296ce-c818-4f83-d1aa-4f2bcb0e2c93"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: semanticscholar in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from semanticscholar) (8.2.3)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from semanticscholar) (0.27.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from semanticscholar) (1.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->semanticscholar) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->semanticscholar) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->semanticscholar) (1.0.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->semanticscholar) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->semanticscholar) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->semanticscholar) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->semanticscholar) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppressing warnings and setting display options\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "# Initializing SemanticScholar\n",
        "sem_sch = SemanticScholar()\n",
        "\n",
        "# Specifying the query\n",
        "query = \"data science\"\n",
        "\n",
        "# Creating a DataFrame to store abstracts\n",
        "df_paper = pd.DataFrame(columns=['abstract'])\n",
        "\n",
        "# Initializing total number of papers fetched\n",
        "total_papers = 0\n",
        "\n",
        "#10,000 abstracts were taking too long to acquire, thus I obtained just 1000 abstracts.\n",
        "\n",
        "# Loop until 1000 abstracts are collected\n",
        "while total_papers < 1000:\n",
        "    # Search for papers related to the query\n",
        "    res = sem_sch.search_paper(query, fields=['abstract'])\n",
        "\n",
        "    # If no results are found, break out of the loop\n",
        "    if not res:\n",
        "        break\n",
        "\n",
        "    # Iterate through each paper in the search results\n",
        "    for paper in res:\n",
        "        # If the paper has an abstract\n",
        "        if paper.abstract:\n",
        "            # Increment the total number of papers fetched\n",
        "            total_papers += 1\n",
        "            # Add the abstract to the DataFrame\n",
        "            df_paper = df_paper.append({'abstract': paper.abstract}, ignore_index=True)\n",
        "\n",
        "        # If 1000 abstracts are collected, break out of the loop\n",
        "        if total_papers == 1000:\n",
        "            break\n",
        "\n",
        "# Save the DataFrame containing abstracts to a CSV file\n",
        "df_paper.to_csv('1000_abstracts.csv', index=False)\n",
        "\n",
        "# Print a message indicating that data collection and saving is complete\n",
        "print(\"Data collection and saving complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4O_4RCtqu_Tg",
        "outputId": "1f641302-7868-4c13-f951-88d8b191d02a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data collection and saving complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to remove non-alphabetic characters\n",
        "def remove_noise(text):\n",
        "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "# Function to remove numbers\n",
        "def remove_numbers(text):\n",
        "    return re.sub(r'\\d+', '', text)\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return ' '.join([word.lower() for word in tokens if word.lower() not in stop_words])\n",
        "\n",
        "# Function to stem text\n",
        "def stem_text(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return ' '.join([stemmer.stem(word) for word in tokens])\n",
        "\n",
        "# Function to lemmatize text\n",
        "def lemmatize_text(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv('1000_abstracts.csv')\n",
        "\n",
        "# Apply preprocessing steps and store them in separate columns\n",
        "df['noise_removed'] = df['abstract'].apply(remove_noise)\n",
        "df['numbers_removed'] = df['noise_removed'].apply(remove_numbers)\n",
        "df['stopwords_removed'] = df['numbers_removed'].apply(remove_stopwords)\n",
        "df['lowercased'] = df['stopwords_removed'].str.lower()\n",
        "df['stemmed'] = df['lowercased'].apply(stem_text)\n",
        "df['lemmatized'] = df['lowercased'].apply(lemmatize_text)\n",
        "\n",
        "# Write the cleaned DataFrame to a new CSV file\n",
        "df.to_csv('abstracts_cleaned.csv', index=False)\n",
        "\n",
        "# Print a message indicating that cleaning and saving is complete\n",
        "print(\"Data cleaning and saving complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Cb6QqqBDYnO",
        "outputId": "cfa83b26-1fcf-45df-c523-11e713d17430"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data cleaning and saving complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Assuming df_clean is already defined and contains the lemmatized column\n",
        "\n",
        "# Selecting the lemmatized column from df_clean\n",
        "df_cleaned_data = df['lemmatized']\n",
        "# Saving the entire cleaned dataframe to a CSV file\n",
        "df.to_csv('df_clean.csv', index=False)\n",
        "# Saving only the lemmatized column to a separate CSV file\n",
        "df_cleaned_data.to_csv('df_cleaned_data.csv', index=False)\n",
        "# Downloading both CSV files using Google Colab's files.download() function\n",
        "files.download(\"df_clean.csv\")\n",
        "files.download(\"df_cleaned_data.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "RcueKay5aI7i",
        "outputId": "b73e1f2f-d33d-463c-c1e9-90afa4b42650"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_63b37f12-1eb5-4126-ad9d-a86eaf121978\", \"df_clean.csv\", 7184257)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0e6097e5-d5a1-4a7e-a18b-71db4bb3a795\", \"df_cleaned_data.csv\", 916272)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d40d4b3-2c4e-448c-a7a9-01dc4de5f8bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constituency Parsing Tree:\n",
            "cambridge compound university PROPN []\n",
            "university compound press NOUN [cambridge]\n",
            "press nsubj let VERB [university]\n",
            "let ROOT let VERB [press, summarize, n, onto, preserve]\n",
            "u nsubj summarize VERB []\n",
            "summarize ccomp let VERB [u, finding]\n",
            "finding xcomp summarize VERB [projection]\n",
            "random amod projection NOUN []\n",
            "projection dobj finding VERB [random, set]\n",
            "set acl projection NOUN []\n",
            "r compound n NOUN []\n",
            "n ccomp let VERB [r]\n",
            "onto prep let VERB [subspace]\n",
            "mdimensional amod subspace NOUN []\n",
            "subspace pobj onto ADP [mdimensional]\n",
            "approximately advmod preserve VERB []\n",
            "preserve ccomp let VERB [approximately, geometry]\n",
            "geometry dobj preserve VERB []\n",
            "\n",
            "Dependency Parsing Tree:\n",
            "cambridge university press let u summarize finding random projection set r n onto mdimensional subspace approximately preserve geometry\n",
            "==================================================\n",
            "Constituency Parsing Tree:\n",
            "chatgpt compound conversational PROPN []\n",
            "conversational nsubj utilizes VERB [chatgpt]\n",
            "ai compound interface PROPN []\n",
            "interface nsubj utilizes VERB [ai]\n",
            "utilizes nsubj stimulate VERB [conversational, interface, algorithm, taking, given, seek, overview, challenge, provide]\n",
            "natural amod language NOUN []\n",
            "language compound machine NOUN [natural]\n",
            "processing compound machine NOUN []\n",
            "machine compound learning VERB [language, processing]\n",
            "learning compound algorithm PROPN [machine]\n",
            "algorithm dobj utilizes VERB [learning]\n",
            "taking xcomp utilizes VERB [buzzword, across, today]\n",
            "world compound storm NOUN []\n",
            "storm compound buzzword NOUN [world]\n",
            "buzzword dobj taking VERB [storm]\n",
            "across prep taking VERB [sector]\n",
            "many amod sector NOUN []\n",
            "sector pobj across ADP [many]\n",
            "today npadvmod taking VERB []\n",
            "given prep utilizes VERB [article]\n",
            "likely amod article NOUN []\n",
            "impact compound model NOUN []\n",
            "model compound perspective NOUN [impact]\n",
            "data compound science NOUN []\n",
            "science compound perspective NOUN [data]\n",
            "perspective compound article NOUN [model, science]\n",
            "article pobj given VERB [likely, perspective]\n",
            "seek conj utilizes VERB [provide]\n",
            "provide dobj seek VERB []\n",
            "overview xcomp utilizes VERB []\n",
            "potential amod challenge NOUN []\n",
            "opportunity compound challenge NOUN []\n",
            "challenge dobj utilizes VERB [potential, opportunity, associated]\n",
            "associated acl challenge NOUN [using]\n",
            "using advcl associated VERB [science]\n",
            "chatgpt compound data NOUN []\n",
            "data compound science NOUN [chatgpt]\n",
            "science dobj using VERB [data]\n",
            "provide xcomp utilizes VERB [advantage]\n",
            "reader compound snapshot NOUN []\n",
            "snapshot compound advantage NOUN [reader]\n",
            "advantage dobj provide VERB [snapshot]\n",
            "stimulate ccomp concludes VERB [utilizes, automating, highlight, provide, languagerelated, generate, addressed]\n",
            "interest compound use NOUN []\n",
            "use compound scientist NOUN [interest]\n",
            "data compound science NOUN []\n",
            "science compound project NOUN [data]\n",
            "project compound chatgpt NOUN [science]\n",
            "paper compound discus NOUN []\n",
            "discus compound chatgpt NOUN [paper]\n",
            "chatgpt compound assist NOUN [project, discus]\n",
            "assist compound scientist NOUN [chatgpt]\n",
            "data compound scientist NOUN []\n",
            "scientist nsubj automating VERB [use, assist, data]\n",
            "automating ccomp stimulate VERB [scientist, workflow]\n",
            "various amod workflow NOUN []\n",
            "aspect compound workflow NOUN []\n",
            "workflow dobj automating VERB [various, aspect, including]\n",
            "including prep workflow NOUN [interpretation]\n",
            "data compound cleaning VERB []\n",
            "cleaning compound interpretation NOUN [data]\n",
            "preprocessing amod training NOUN []\n",
            "model compound training NOUN []\n",
            "training compound result NOUN [preprocessing, model]\n",
            "result compound interpretation NOUN [training]\n",
            "interpretation pobj including VERB [cleaning, result]\n",
            "also advmod highlight VERB []\n",
            "highlight conj stimulate VERB [also]\n",
            "chatgpt compound potential NOUN []\n",
            "potential nsubj provide VERB [chatgpt]\n",
            "provide conj stimulate VERB [potential, improve]\n",
            "new amod insight NOUN []\n",
            "insight compound improve VERB [new]\n",
            "improve dobj provide VERB [insight, decisionmaking]\n",
            "decisionmaking prep improve VERB [process]\n",
            "process dobj decisionmaking VERB [analyzing]\n",
            "analyzing acl process NOUN [architecture]\n",
            "unstructured amod architecture NOUN []\n",
            "data compound examine NOUN []\n",
            "examine compound chatgpts PROPN [data]\n",
            "advantage compound chatgpts PROPN []\n",
            "chatgpts compound architecture NOUN [examine, advantage]\n",
            "architecture dobj analyzing VERB [unstructured, chatgpts, including]\n",
            "including prep architecture NOUN [ability]\n",
            "ability pobj including VERB [finetuned]\n",
            "finetuned acl ability NOUN []\n",
            "wide amod range NOUN []\n",
            "range nsubj languagerelated VERB [wide]\n",
            "languagerelated conj stimulate VERB [range, task]\n",
            "task dobj languagerelated VERB []\n",
            "generate advcl stimulate VERB []\n",
            "synthetic amod issue NOUN []\n",
            "data compound limitation NOUN []\n",
            "limitation compound issue NOUN [data]\n",
            "issue nsubj addressed VERB [synthetic, limitation]\n",
            "also advmod addressed VERB []\n",
            "addressed conj stimulate VERB [issue, also, around]\n",
            "particularly advmod around ADP []\n",
            "around prep addressed VERB [particularly, plagiarism]\n",
            "concern compound plagiarism NOUN []\n",
            "bias compound plagiarism NOUN []\n",
            "plagiarism pobj around ADP [concern, bias, using]\n",
            "using acl plagiarism NOUN [chatgpt]\n",
            "chatgpt dobj using VERB []\n",
            "overall amod paper NOUN []\n",
            "paper nsubj concludes VERB [overall]\n",
            "concludes ROOT concludes VERB [stimulate, paper, enhance]\n",
            "benefit compound cost NOUN []\n",
            "outweigh compound cost NOUN []\n",
            "cost compound potential NOUN [benefit, outweigh]\n",
            "chatgpt compound potential NOUN []\n",
            "potential nsubj enhance VERB [cost, chatgpt]\n",
            "greatly advmod enhance VERB []\n",
            "enhance ccomp concludes VERB [potential, greatly, become]\n",
            "productivity compound accuracy NOUN []\n",
            "accuracy compound workflow NOUN [productivity]\n",
            "data compound workflow NOUN []\n",
            "science compound workflow NOUN []\n",
            "workflow nsubj become VERB [accuracy, data, science]\n",
            "likely advmod become VERB []\n",
            "become ccomp enhance VERB [workflow, likely, important, assist, resource, compared, finetuned]\n",
            "increasingly advmod important ADJ []\n",
            "important acomp become VERB [increasingly]\n",
            "tool compound field NOUN []\n",
            "intelligence compound augmentation NOUN []\n",
            "augmentation compound field NOUN [intelligence]\n",
            "field compound data NOUN [tool, augmentation]\n",
            "data compound chatgpt PROPN [field]\n",
            "science compound chatgpt PROPN []\n",
            "chatgpt nsubj assist VERB [data, science]\n",
            "assist ccomp become VERB [chatgpt, science]\n",
            "wide amod range NOUN []\n",
            "range nmod language NOUN [wide]\n",
            "natural amod language NOUN []\n",
            "language nmod science NOUN [range, natural]\n",
            "processing compound science NOUN []\n",
            "task compound data NOUN []\n",
            "data compound science NOUN [task]\n",
            "science dobj assist VERB [language, processing, data, including]\n",
            "including prep science NOUN [classification]\n",
            "language compound translation NOUN []\n",
            "translation compound sentiment NOUN [language]\n",
            "sentiment compound classification NOUN [translation]\n",
            "analysis compound classification NOUN []\n",
            "text compound classification NOUN []\n",
            "classification pobj including VERB [sentiment, analysis, text]\n",
            "however advmod chatgpt PROPN []\n",
            "chatgpt compound resource NOUN [however]\n",
            "save compound resource NOUN []\n",
            "time compound resource NOUN []\n",
            "resource attr become VERB [chatgpt, save, time]\n",
            "compared prep become VERB []\n",
            "training compound scratch NOUN []\n",
            "model compound scratch NOUN []\n",
            "scratch nsubj finetuned VERB [training, model]\n",
            "finetuned conj become VERB [scratch, perform]\n",
            "specific amod case NOUN []\n",
            "use compound case NOUN []\n",
            "case nsubj perform VERB [specific, use]\n",
            "may aux perform VERB []\n",
            "perform advcl finetuned VERB [case, may, task, pose]\n",
            "well advmod certain ADJ []\n",
            "certain amod task NOUN [well]\n",
            "task dobj perform VERB [certain]\n",
            "specifically advmod trained VERB []\n",
            "trained amod chatgpt NOUN [specifically, additionally]\n",
            "additionally advmod trained VERB []\n",
            "output compound chatgpt NOUN []\n",
            "chatgpt nsubj pose VERB [trained, output]\n",
            "may aux pose VERB []\n",
            "difficult amod interpret NOUN []\n",
            "interpret nsubj pose VERB [difficult]\n",
            "could aux pose VERB []\n",
            "pose advcl perform VERB [chatgpt, may, interpret, could, challenge, decisionmaking]\n",
            "challenge dobj pose VERB []\n",
            "decisionmaking xcomp pose VERB [application]\n",
            "data compound science NOUN []\n",
            "science compound application NOUN [data]\n",
            "application dobj decisionmaking VERB [science]\n",
            "\n",
            "Dependency Parsing Tree:\n",
            "chatgpt conversational ai interface utilizes natural language processing machine learning algorithm taking world storm buzzword across many sector today given likely impact model data science perspective article seek provide overview potential opportunity challenge associated using chatgpt data science provide reader snapshot advantage stimulate interest use data science project paper discus chatgpt assist data scientist automating various aspect workflow including data cleaning preprocessing model training result interpretation also highlight chatgpt potential provide new insight improve decisionmaking process analyzing unstructured data examine advantage chatgpts architecture including ability finetuned wide range languagerelated task generate synthetic data limitation issue also addressed particularly around concern bias plagiarism using chatgpt overall paper concludes benefit outweigh cost chatgpt potential greatly enhance productivity accuracy data science workflow likely become increasingly important tool intelligence augmentation field data science chatgpt assist wide range natural language processing task data science including language translation sentiment analysis text classification however chatgpt save time resource compared training model scratch finetuned specific use case may perform well certain task specifically trained additionally output chatgpt may difficult interpret could pose challenge decisionmaking data science application\n",
            "==================================================\n",
            "                                            lemmatized  noun  verb  adjective  adverb  \\\n",
            "0    cambridge university press let u summarize fin...     9     2          5       1   \n",
            "1    chatgpt conversational ai interface utilizes n...    96    33         31      10   \n",
            "2    communication website httpscacmacmorg feature ...    22     5          2       2   \n",
            "3    r programming language approaching th birthday...    52    15         18       3   \n",
            "4    recent decade healthcare organization around w...    64    10         19       6   \n",
            "..                                                 ...   ...   ...        ...     ...   \n",
            "994  desire predict discoveriesto idea advance disc...    26     6         11       1   \n",
            "995  examine intersection fair principle findable a...    55    12         34       2   \n",
            "996  year heavy regulation bureaucratic inefficienc...    83    26         28       1   \n",
            "997  largescale citizenscience project atlas specie...    95    30         35       9   \n",
            "998  national cancer institute nci designated topic...    44    11          9       1   \n",
            "\n",
            "                                              entities  \n",
            "0                         [cambridge university - ORG]  \n",
            "1                                       [today - DATE]  \n",
            "2    [httpscacmacmorg feature dozen blogger - ORG, ...  \n",
            "3    [three decade - DATE, ten thousand - CARDINAL,...  \n",
            "4    [recent decade - DATE, three - CARDINAL, three...  \n",
            "..                                                 ...  \n",
            "994                                                 []  \n",
            "995                                                 []  \n",
            "996  [healthcare restore agency - ORG, log easy acc...  \n",
            "997                                                 []  \n",
            "998              [national cancer institute nci - ORG]  \n",
            "\n",
            "[999 rows x 6 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "import spacy\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Load SpaCy English model for Named Entity Recognition\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Read the CSV file containing the clean text\n",
        "try:\n",
        "    df = pd.read_csv('df_cleaned_data.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"File not found. Please ensure the file path is correct.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(\"An error occurred while reading the CSV file:\", e)\n",
        "    exit()\n",
        "\n",
        "# Function to conduct Parts of Speech (POS) Tagging and calculate counts\n",
        "def pos_tagging_and_count(text):\n",
        "    nouns = verbs = adjectives = adverbs = 0\n",
        "    if isinstance(text, str):  # Check if text is a string\n",
        "        tokens = word_tokenize(text)\n",
        "        pos_tags = pos_tag(tokens)\n",
        "        for _, tag in pos_tags:\n",
        "            if tag.startswith('N'):  # Noun\n",
        "                nouns += 1\n",
        "            elif tag.startswith('V'):  # Verb\n",
        "                verbs += 1\n",
        "            elif tag.startswith('J'):  # Adjective\n",
        "                adjectives += 1\n",
        "            elif tag.startswith('R'):  # Adverb\n",
        "                adverbs += 1\n",
        "    return nouns, verbs, adjectives, adverbs\n",
        "\n",
        "# Function to perform Constituency Parsing and Dependency Parsing\n",
        "def parse_trees(text):\n",
        "    if isinstance(text, str):  # Check if text is a string\n",
        "        sentences = sent_tokenize(text)\n",
        "        for sentence in sentences:\n",
        "            print(\"Constituency Parsing Tree:\")\n",
        "            parsed_sentence = nlp(sentence)\n",
        "            for token in parsed_sentence:\n",
        "                print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
        "                      [child for child in token.children])\n",
        "            print(\"\\nDependency Parsing Tree:\")\n",
        "            print(parsed_sentence)\n",
        "            print(\"=\"*50)\n",
        "\n",
        "# Function to perform Named Entity Recognition (NER) and calculate counts\n",
        "def ner_and_count(text):\n",
        "    entities = []\n",
        "    if isinstance(text, str):  # Check if text is a string\n",
        "        doc = nlp(text)\n",
        "        for ent in doc.ents:\n",
        "            entities.append(ent.text + \" - \" + ent.label_)\n",
        "    return entities\n",
        "\n",
        "# Apply functions to each row of the DataFrame\n",
        "df['noun'], df['verb'], df['adjective'], df['adverb'] = zip(*df['lemmatized'].apply(pos_tagging_and_count))\n",
        "\n",
        "df['lemmatized'].head(2).apply(parse_trees)\n",
        "#df['lemmatized'].apply(parse_trees)\n",
        "df['entities'] = df['lemmatized'].apply(ner_and_count)\n",
        "\n",
        "# Display DataFrame with added columns\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your response below\n",
        "'''\n",
        "to be honest, This assignment was very very tough. It tooks 7-8 hours to complete it. I had to use many references for this assignment.\n",
        "IT WAS VERY TOUGH FOR ME.\n",
        "I founded challenges in Question 2 and 3.\n",
        "However, question 1 was easy.\n",
        "10,000 abstracts were taking too long to acquire, thus I obtained just 1000 abstracts\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "_e557s2w4BpK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "cd2d933e-15c5-4286-de2f-dde2d5229296"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nto be honest, This assignment was very very tough. It tooks 7-8 hours to complete it. I had to use many references for this assignment. \\nIT WAS VERY TOUGH FOR ME.\\nI founded challenges in Question 2 and 3.\\nHowever, question 1 was easy.\\n10,000 abstracts were taking too long to acquire, thus I obtained just 1000 abstracts\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}