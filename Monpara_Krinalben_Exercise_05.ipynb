{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KrinalM/Krinalben_INFO5731_Spring2024/blob/main/Monpara_Krinalben_Exercise_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 5**\n",
        "\n",
        "**This exercise aims to provide a comprehensive learning experience in text analysis and machine learning techniques, focusing on both text classification and clustering tasks.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU-pLW33lpcS"
      },
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## **Question 1 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text classification** as well as the performance evaluation. In addition, you are requried to conduct **10 fold cross validation** (https://scikit-learn.org/stable/modules/cross_validation.html) in the training.\n",
        "\n",
        "\n",
        "\n",
        "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loi8Sh7UE6ha"
      },
      "source": [
        "**Algorithms:**\n",
        "\n",
        "*   MultinominalNB\n",
        "*   SVM\n",
        "*   KNN\n",
        "*   Decision tree\n",
        "*   Random Forest\n",
        "*   XGBoost\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "**Evaluation measurement:**\n",
        "\n",
        "\n",
        "*   Accuracy\n",
        "*   Recall\n",
        "*   Precison\n",
        "*   F-1 score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GJ1aa8UiAuEd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from gensim.models import Word2Vec\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.model_selection import train_test_split, cross_validate, KFold\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P62S6P2ZAwmA",
        "outputId": "2e88af4e-95ea-4f15-f7d7-66f6d0c81f50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text/csv; charset=iso-8859-1\n"
          ]
        }
      ],
      "source": [
        "!file -bi /content/stsa-train.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mc_5FWVKAyG_"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv(\"/content/stsa-train.csv\", encoding=\"latin-1\")\n",
        "test_data = pd.read_csv(\"/content/stsa-test.csv\", encoding=\"latin-1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TIehew4OAzkK"
      },
      "outputs": [],
      "source": [
        "# Split data into features and labels\n",
        "X_train = train_data['text']\n",
        "y_train = train_data['sentiment']\n",
        "X_test = test_data['text']\n",
        "y_test = test_data['sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KXn-FyD2A1TW"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X, y):\n",
        "    scoring = ['accuracy', 'recall', 'precision', 'f1']\n",
        "    cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "    scores = cross_validate(model, X, y, cv=cv, scoring=scoring)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cg5HA9a8A23E"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate(model, model_name):\n",
        "    print(\"Training and evaluating\", model_name)\n",
        "    scores = evaluate_model(model, X_train_counts, y_train)\n",
        "    print(\"Mean accuracy:\", scores['test_accuracy'].mean())\n",
        "    print(\"Mean recall:\", scores['test_recall'].mean())\n",
        "    print(\"Mean precision:\", scores['test_precision'].mean())\n",
        "    print(\"Mean F1 score:\", scores['test_f1'].mean())\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "M7x7DkTGA4c3"
      },
      "outputs": [],
      "source": [
        "# Convert text data into numerical features\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_counts = vectorizer.fit_transform(X_train)\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys_GiPYVA6WX",
        "outputId": "175ed64f-1dc2-4156-c57b-4660878befb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and evaluating Multinomial Naive Bayes\n",
            "Mean accuracy: 0.7917630057803468\n",
            "Mean recall: 0.8020372659413895\n",
            "Mean precision: 0.7994481135510914\n",
            "Mean F1 score: 0.8005499641877032\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Multinomial Naive Bayes\n",
        "nb_model = MultinomialNB()\n",
        "train_and_evaluate(nb_model, \"Multinomial Naive Bayes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPMIWdPTBUWO",
        "outputId": "ccfaba93-d2f5-45f1-b2ac-7622cb984f09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and evaluating K-Nearest Neighbors\n",
            "Mean accuracy: 0.5773121387283238\n",
            "Mean recall: 0.5676928908737323\n",
            "Mean precision: 0.6007389224930721\n",
            "Mean F1 score: 0.5827839306081636\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# K-Nearest Neighbors\n",
        "knn_model = KNeighborsClassifier()\n",
        "train_and_evaluate(knn_model, \"K-Nearest Neighbors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9DaYvc_BcrA",
        "outputId": "45810c1b-bfa8-4b26-8a78-f2ac91177875"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and evaluating Decision Tree\n",
            "Mean accuracy: 0.6377167630057803\n",
            "Mean recall: 0.6564791643511114\n",
            "Mean precision: 0.6515396882332904\n",
            "Mean F1 score: 0.6535907393263543\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Decision Tree\n",
        "dt_model = DecisionTreeClassifier()\n",
        "train_and_evaluate(dt_model, \"Decision Tree\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tgFJnJuNBjRk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bb212ef-e997-445d-90b7-7860e59075ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and evaluating XGBoost\n",
            "Mean accuracy: 0.7199421965317919\n",
            "Mean recall: 0.7895318141081059\n",
            "Mean precision: 0.7084570903552997\n",
            "Mean F1 score: 0.7462264636758384\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# XGBoost\n",
        "xgb_model = XGBClassifier()\n",
        "train_and_evaluate(xgb_model, \"XGBoost\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jBr30Iw3Bg4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ac7c603-c410-4e30-e978-3e73bcf5ec87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and evaluating Random Forest\n",
            "Mean accuracy: 0.733092485549133\n",
            "Mean recall: 0.7780157325752171\n",
            "Mean precision: 0.7290991217104332\n",
            "Mean F1 score: 0.7524772120269957\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Random Forest\n",
        "rf_model = RandomForestClassifier()\n",
        "train_and_evaluate(rf_model, \"Random Forest\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "a17V34xWBO7Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac348752-ab04-4285-a646-c76df7e1742a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and evaluating Support Vector Machine\n",
            "Mean accuracy: 0.7658959537572254\n",
            "Mean recall: 0.7786934520164545\n",
            "Mean precision: 0.7739686876477824\n",
            "Mean F1 score: 0.7759716841790995\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Support Vector Machine\n",
        "svm_model = SVC(kernel='linear')\n",
        "train_and_evaluate(svm_model, \"Support Vector Machine\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "u5cOtHctB60X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54137cd7-efd0-4be9-e1a1-54bad5f824f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8116419549697969\n",
            "Test Recall: 0.8305830583058306\n",
            "Test Precision: 0.7997881355932204\n",
            "Test F1 score: 0.8148947652455478\n"
          ]
        }
      ],
      "source": [
        "# Evaluate best model on test data\n",
        "best_model = svm_model  # Change this to the best performing model\n",
        "X_test_counts = vectorizer.transform(X_test)\n",
        "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
        "best_model.fit(X_train_tfidf, y_train)\n",
        "y_pred = best_model.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate evaluation metrics on test data\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Test Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"Test Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Test F1 score:\", f1_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## **Question 2 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text clustering**.\n",
        "\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "**Apply the listed clustering methods to the dataset:**\n",
        "*   K-means\n",
        "*   DBSCAN\n",
        "*   Hierarchical clustering\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "You can refer to of the codes from  the follwing link below.\n",
        "https://www.kaggle.com/karthik3890/text-clustering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ckxio9ydpGTA",
        "outputId": "367e3cc5-ce58-45c7-c1a2-91659c4d5b51"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/171.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m163.8/171.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.40.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 sentence-transformers-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from gensim.models import Word2Vec\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import nltk\n",
        "from nltk.cluster import KMeansClusterer\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "9xWRODyWo-g-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/Amazon_Unlocked_Mobile.csv\")\n",
        "\n",
        "# Drop rows with missing review text\n",
        "df = df.dropna(subset=['Reviews'])"
      ],
      "metadata": {
        "id": "JpXwbES_pBGl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    words = word_tokenize(text)\n",
        "    words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "df['Cleaned text'] = df['Reviews'].apply(preprocess_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6L0YOm8pLhm",
        "outputId": "1878f655-4130-4e2b-a4a5-ac1f4492f918"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply K-means clustering\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['Cleaned text'])\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "kmeans.fit(X)\n",
        "df['Kmeans cluster'] = kmeans.labels_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDO0NSDXpS7D",
        "outputId": "7707de16-ac3a-4a0b-d7dc-fd084e70944d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "df['DBSCAN cluster'] = dbscan.fit_predict(X)"
      ],
      "metadata": {
        "id": "i2tWGKZipc3J"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Word2Vec model\n",
        "sentences = [word_tokenize(text) for text in df['Cleaned text']]\n",
        "word2vec_model = Word2Vec(sentences, min_count=1)"
      ],
      "metadata": {
        "id": "rXDZRTpYrAaD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[['Reviews', 'Kmeans cluster', 'DBSCAN cluster']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WtFwPiJr8d7",
        "outputId": "71781a9b-4f15-43f0-c3a9-044b3e817381"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 Reviews  Kmeans cluster  \\\n",
            "0      I feel so LUCKY to have found this used (phone...               4   \n",
            "1      nice phone, nice up grade from my pantach revu...               4   \n",
            "2                                           Very pleased               4   \n",
            "3      It works good but it goes slow sometimes but i...               0   \n",
            "4      Great phone to replace my lost phone. The only...               4   \n",
            "...                                                  ...             ...   \n",
            "38448  First time I received the phone from the selle...               4   \n",
            "38449                                         Works well               4   \n",
            "38450  Probably could've found better prices , also t...               4   \n",
            "38451    The iphone is unlock Good phone but bad ime att               4   \n",
            "38452  Don't buy it. Yes the iPhone is brand new and ...               4   \n",
            "\n",
            "       DBSCAN cluster  \n",
            "0                  -1  \n",
            "1                  -1  \n",
            "2                   0  \n",
            "3                  -1  \n",
            "4                  -1  \n",
            "...               ...  \n",
            "38448            1834  \n",
            "38449              52  \n",
            "38450            1835  \n",
            "38451            1836  \n",
            "38452            1711  \n",
            "\n",
            "[38453 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRijW2aLGONl"
      },
      "source": [
        "**In one paragraph, please compare the results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIYCj5qyGfSL"
      },
      "source": [
        "**Write your response here:**\n",
        "\n",
        "K-means is efficient and easy to implement, providing clear cluster boundaries, but it assumes clusters of similar size and shape and requires the number of clusters to be specified beforehand. DBSCAN, on the other hand, is robust to noise and can find clusters of arbitrary shapes and sizes without requiring the number of clusters as input, but it struggles with varying density and high-dimensional data. Word2Vec embeddings capture semantic similarities between words, enabling K-means to cluster documents based on word meanings; however, it may not capture context well. Therefore, while K-means with Word2Vec offers interpretability, it may overlook syntactic nuances. BERT embeddings, although powerful in capturing context and semantic relationships, require significant computational resources, limiting their practicality in resource-constrained environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEs-OoDEhTW4"
      },
      "source": [
        "# Mandatory Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUKC7suYhVl0"
      },
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "CAq0DZWAhU9m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "491413b9-2dde-4e45-bfc2-8fd6f7f7090d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThis excerise is not much hard as it is. For question 2, The listed clustering methods:DBSCAN,Hierarchical clustering,Word2Vec taking too much time for run.\\nBut I learnt a lot from this exercise. It took many hours to complete this.\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "This excerise is not much hard as it is. For question 2, The listed clustering methods:DBSCAN,Hierarchical clustering,Word2Vec taking too much time for run.\n",
        "But I learnt a lot from this exercise. It took many hours to complete this.\n",
        "\n",
        "\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}